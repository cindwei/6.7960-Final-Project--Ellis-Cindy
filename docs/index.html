<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 70%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", "Avenir", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", "Avenir", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", "Avenir", sans-serif;
			font-size: 14px;
			width: 25%; /* Change this percentage as needed */
			max-width: 256px; /* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;  /* Optional: Adds padding inside the caption */
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

</style>

	  <title>The Platonic Representation Hypothesis</title>
      <meta property="og:title" content="The Platonic Representation Hypothesis" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Neurodevelopmentally Inspired Pruning</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Cindy Wei</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Ellis Chae</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for MIT 6.7960</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
			  <a href="#background">Background </a><br><br>
			  <a href="#hypothesis">Hypothesis </a><br><br>
			  <a href="#methods">Methods</a><br><br>
			  <a href="#analysis">Results and Analysis</a><br><br>
              <a href="#implications_and_limitations">Implications and Limitations</a><br><br>
			  <a href="#implications_and_limitations">Future</a><br><br>
          </div>
				</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		    <div class="margin-right-block">
						Caption for the image.
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
            <p>Modern deep learning models are heavily overparameterized, often containing more weights and neurons than are needed to solve a task. This leads to unnecessary computation, memory usage, and energy costs. In contrast, biological neural systems such as the human brain achieve remarkable efficiency. During development and learning, the brain prunes both entire neurons and individual synapses, strengthens relevant connections, and maintains sparse architectures that still support flexible and powerful computation. Importantly, this pruning happens throughout learning, not only before or after it.</p>
				<p>Biologically-inspired pruning for neural nets is a common idea. The biological process has two key components.</p>

				<p>First, entire-neuron pruning occurs early in life, removing whole units. According to neurotrophic theory, the brain first overproduces neurons early in life. Then, developing neurons compete for a limited supply of neurotrophic factor (NTF) hormone.</p>
				<p>Second, synaptic pruning continues throughout adolescence and adulthood. Instead of removing entire neurons, only a subset of the junctions between them are removed. This fine-tunes the connectivity of surviving neurons by eliminating weak or redundant synapses. </p>
				</p>In deep learning, these parallel naturally onto structured pruning (removing entire neurons or channels) and unstructured pruning (permanently zeroing individual weights). Structured pruning reduces inference cost because it simplifies the actual dimensions of the model, whereas unstructured pruning typically requires specialized hardware to speed up computation as it does not intrinsically reduce the number of operations.
				</p>Although biological systems use both pruning strategies together and adjust connections dynamically during learning, most deep learning approaches do not. Many pruning methods operate either before training or after training, and dynamic pruning during training is far less explored. Even methods that do prune during training rarely incorporate biologically grounded rules such as activity-based or energy-based selection. As a result, we still do not know whether using biologically inspired pruning signals during training can produce models that are more efficient or more accurate than those produced by standard pruning techniques.</p>
				</p>To address this question, we implement three biologically motivated mechanisms: unstructured magnitude pruning, energy-based and activation-based structured pruning. We compare two ways of scoring neurons for structured pruning, inspired by how the brain considers both ATP usage and activation frequency.</p>
				</p>These choices reflect the two major forms of pruning observed in the brain and allow us to test whether combining structured and unstructured mechanisms during training yields better results than either method alone. We evaluate all methods consistently on CIFAR-10 to assess sparsity–accuracy trade-offs, training dynamics, and representational changes.</p>
				</p>Ultimately, our goal is to explore whether the principles that guide the efficiency of biological brains can also guide the design of more compact and computationally efficient deep learning architectures.</p>
		    </div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>
	<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Background</h1>
				  	</p>Pruning is a long studied strategy for reducing the size and computational cost of neural networks. A growing body of work shows that many modern neural net parameters are unnecessary for achieving strong performance. This has led to vast literature on identifying smaller subnetworks that retain the accuracy of the full model, often without requiring substantial changes to training pipelines. </p>
				</p>One of the foundational ideas in this area is the Lottery Ticket Hypothesis. Frankle and Carbin (2018) showed that dense, randomly initialized neural networks contain subnetworks (“winning tickets”) that can be trained to reach the same accuracy as the original large model. These winning subnetworks are found through iterative magnitude pruning followed by retraining using only the surviving weights, and these subnetworks can match the accuracy of the full model while training in as few or fewer epochs than the original network.</p> 
				</p>Follow up work has extended this idea by demonstrating that, in many overparameterized models, subnetworks with competitive performance can be identified even without training. Recent studies show that optimization procedures can be used to select an efficient subnetwork directly from the initial weights of the model. This suggests that overparameterization creates a large space of viable subnetworks, many of which are structurally efficient.</p>
				</p>Research in pruning has since expanded toward a wide range of techniques. Pruning methods can be grouped into several major patterns such as loss-based sparsification which encourages sparse weights through modified objectives, sparse-to-sparse approaches that prune and regrow weights during training, filter-level scoring methods for structured pruning, and differentiable mask-learning techniques. These pruning methods are less explored than post-training approaches due to their more complex pruning-regrowth dynamics. Most of the literature has also focused on convolutional networks with considerably fewer studies examining pruning in MLPs or transformers. </p>
				</p>Within unstructured pruning, magnitude pruning remains one of the strongest and simplest baselines. However, more advanced methods exist. For example, Wanda integrates both weights and activation statistics to determine which weights to remove and often outperforms standard magnitude pruning. SparseGPT uses a local reconstruction loss to prune weights while preserving the function of the model, achieving high sparsity with minimal accuracy loss in large language models. At the structured level, methods such as coupled structural pruning identify groups of neurons or channels that must be removed together due to architectural dependencies, achieving substantial pruning rates with modest accuracy drops. </p>
				</p>Dynamic pruning during training has also been explored but less extensively. Methods such as RigL and SET prune and regrow connections based on weight magnitude or gradient information. While these approaches introduce sparsity throughout training, they do not perform true structural pruning and still rely heavily on magnitude signals rather than higher-level criteria. </p>
				</p>There remains substantial interest in pruning strategies that are simple and effective during training. In biological neural systems, connections are strengthened, weakened, or removed through mechanisms such as synaptic decay, activity-dependent suppression, and Hebbian plasticity. These principles motivate our exploration of energy-based pruning, activity-based pruning, and magnitude pruning. By examining how these mechanisms behave under the same training pipeline, we aim to highlight similarities and differences between biological rules and modern pruning strategies. </p>
				<br><br>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

	<div class="content-margin-container" id="hypothesis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Hypothesis</h1>
					<p>Our hypothesis is that during training structural pruning will allow the model to keep the neuron structures that matter most, which should reduce computational cost and help preserve accuracy even at small model sizes. We also expect that combining this structured pruning with unstructured magnitude pruning will be even more effective, since each method removes a different type of redundancy in the network. 
					</p>We predict that a model that prunes itself while it trains can achieve better accuracy-to-size trade-offs than a small model trained from scratch.</p>
					</p>
				<br><br>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

	<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Methods</h1>
						We follow a unified experimental pipeline designed to compare a small baseline model with three methods: magnitude pruning, energy-based structured pruning, and activation-based structured pruning. All models are trained under identical conditions, and every pruning method follows the same pruning schedule to ensure a controlled comparison. <br><br>
						<p><b>General Architecture</b></p>
						<p> All experiments use a custom neural network adapted from the CIFAR-10 model in Homework 1. We chose a simplified architecture to maximize customizability and ensure realistic runtimes on a single T4 GPU in Colab. The model is intentionally made very small so that the effects of parameter reduction are easy to observe. </p>
						<ul>
							<li>an input layer of input size (3072) → 2</b> </li>
							<li>a hidden layer mapping 2 → 2</b> </li>
							<li>Ba classification layer mapping 2 → 10.</li>
						</ul>
						<p>This architecture contains 6168 parameters.<p>
						</p>The starting network (before pruning) was calculated in order for pruning to produce the target network size after the fixed training schedule. </p>
						<p></b>The initial network consists of:</b></p>
							<li>an input layer of input size (3072) → 20</li>
							</li>a hidden layer mapping 20 → 20</li>
							</li>a classification layer mapping 20 → 10.</li>
						<p><b>Training Configuration</b></p>
						<p>All models are trained for 30 epochs, with the first 5 epochs serving as a warm-up period with no pruning. From epochs 7 through 30, pruning occurs once after every epoch. Trainings use:</p>
						<ul>
							<li>Optimizer: <b>Stochastic Gradient Descent (HW1)</b> </li>
							<li>Batch Size: <b>512</b></li>
							<li> Loss Function: <b>Cross Entropy Loss (HW1)</b></li>
							<li> Learning Rate: <b>.15</b></li>
						</ul>
						</p>
						<p>We use a pruning scheduler that computes the pruning ratio for each epoch. This is calculated as # of current epoch - # of warmup epochs / (total epochs - # of warmup epochs -1). After the warm-up period, the prune ratio increases linearly from 0 to the final target sparsity of 90%, ensuring smooth parameter removal over time.<p>
						<b>General Pruning Mechanics</b>
						</p>FLOPs are estimated as:</p>
						<div style="padding-left: 40px;">
    						</p>FLOPs=2×(in features)×(out features)</p>
						</div>
						</p>summed across all layers.</p>
						<b>Neuron and Weight Importance</b>
						<ul>
							<li>Unstructured pruning ranks individual weights by absolute value.</li>
							<li>Structured pruning ranks neurons by the L1 norm of their weights.</li> 
							<li>Importance scores are recalculated after every epoch during the pruning phase.</li>
						</ul>
						</p><b>Masking Mechanism</b></p>
						<p>We store a binary mask for weights or neurons.</p>
						<p> Both weights and gradients are masked to prevent regrowth, ensuring that once a parameter is pruned, it stays pruned.</p>
						<b>Sparsity Tracking</b>
						<ul>
							<li>Magnitude pruning: sparsity = (# zero weights) / (total weights).</li>
							<li>Structured pruning: sparsity = (current parameter count) / (original parameter count).</li>
						</ul>
						<p><b>Method-Specific Architecture</b></p>
						<p><b>Baseline</b></p>
						<p>The target network trained for 30 epochs without pruning.</p>
						<p><b>Magnitude</b></p>
						<p>Magnitude pruning identifies the threshold corresponding to the k-th smallest value, where k=(total weights)×(current prune ratio). All weights below this threshold are zeroed and masked. </p>
						<p><b>Energy</b></p>
						<p>For each neuron, we compute energy as the mean-squared value of the row of weights:</p>
						<div style="padding-left: 40px;">
    						</p>E_i=1/n∑j wij^2</p>
						</div>
						<p>Neurons are ranked by this energy score, and pruned again according to a kth value threshold.</p>
						<p>When a neuron is removed:</p>
						<ul>
							<li>We remove its entire weight row.</li>
							<li>We remove the corresponding column input to the next layer, etc.</li>
						</ul>
						<p>We prune only the first two layers to avoid altering the classification layer.</p>
						<p><b>Activation-Based Structured Pruning</b></p>
						<p>This method is the same as energy-based except it instead ranks neurons by how strongly they activate during training. For each neuron, we compute an activation score:</p>
						<div style="padding-left: 40px;">
    						</p>A_j = Σ_{i=1..d_in} ( (1/B) · Σ_{b=1..B} (x_i^(b))² ) · |W[j,i]|</p>
						</div>
						<p>which measures how strongly the neuron responds to the input using its current weight vector. </p>
		    </div>		
		</div>

	<div class="content-margin-container" id="Results and Analysis">
    	<div class="margin-left-block"></div>

		<div class="main-content-block">
			<h1>Results & Analysis</h1>
			<p></p>
			<div class="image-row">
				<img src="./images/table.png" width="512">
				<div class="caption">Caption for the image.</div>
			</div>
			<!-- Image 1 + caption -->
			<div class="image-row">
				<img src="./images/accuracies.png" width="512">
				<div class="caption">Caption for the image.</div>
			</div>

			<!-- Image 2 + caption -->
			<div class="image-row">
				<img src="./images/flops.png" width="512">
				<div class="caption">Caption for the image.</div>
			</div>
			<!-- Image 3 + caption -->
			<div class="image-row">
				<img src="./images/neuronsurvival.png" width="512">
				<div class="caption">Caption for the image.</div>
			</div>
		</div>

		<div class="margin-right-block"></div>
	</div>

	<div class="content-margin-container" id="magnitude_based">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and Limitations</h1>
            <p></p>
		    </div>
		</div>

	<div class="content-margin-container" id="magnitude_based">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Future</h1>
            <p></p>
		    </div>
		</div>

	</body>

</html>
