<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #f5f9ff;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: left; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 85%; /* Change this percentage as needed */
    max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		border-left: 1px solid #DDD;
		border-right: 1px solid #DDD;
		padding: 8px 8px 8px 8px;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", "Avenir", sans-serif;
	}
	.margin-left-block {
			font-size: 14px;
			width: 15%; /* Change this percentage as needed */
			max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", "Avenir", sans-serif;
			padding: 5px;
	}
	.margin-right-block {
			display:none;
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: #0e7862; /*#1367a7;*/
		text-decoration: none;
	}
	a:hover {
		color: #24b597; /*#208799;*/
	}

	h1 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 10px;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 70%;
    max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.citation {
    color: #9bbcff;
    cursor: pointer;
    text-decoration: none;
    position: relative;
	}

	.citation-popup {
		display: none;
		position: absolute;
		background: #1e1e1e;
		color: #ffffff;
		padding: 12px 16px;
		border-radius: 8px;
		box-shadow: 0px 4px 15px rgba(0,0,0,0.35);
		width: 320px;
		z-index: 999;
		top: 1.5em;   /* adjust */
		left: 0;      /* adjust */
	}

	/* Show popup when hovering over the citation */
	.citation:hover  .citation-popup {
		display: block;
	}

</style>

	  <title>Neurodevelopmentally Inspired Pruning</title>
      <meta property="og:title" content="Neurodevelopmentally Inspired Pruning" />
			<meta charset="UTF-8">
		<script>
    		window.MathJax = {
      		tex: {
        		inlineMath: [['$', '$'], ['\\(', '\\)']],
        		displayMath: [['$$', '$$'], ['\\[', '\\]']]
      		},
      		svg: { fontCache: 'global' }
    	};
  	</script>
  	<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=left>
								<tr>
									<td colspan=4>
										<span style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">Neurodevelopmentally Inspired Pruning</span>
									</td>
								</tr>
								<tr>
										<td align=left>
												<span style="font-size:17px"><a href="your_website">Cindy Wei</a></span>
										</td>
										<td align=left>
												<span style="font-size:17px"><a href="your_partner's_website">Ellis Chae</a></span>
										</td>
								<tr>
									<td colspan=4 align=left><span style="font-size:18px">Final project for MIT 6.7960</span></td>
								</tr>
						</table>
					</div>
					<div class="margin-right-block">
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
              <b style="font-size:16px">Outline</b><br><br>
              <a href="#intro">Introduction</a><br><br>
			  <a href="#background">Background </a><br><br>
			  <a href="#hypothesis">Hypothesis </a><br><br>
			  <a href="#methods">Methods</a><br><br>
			  <a href="#method-specific-architecture">Method-Specific Architecture</a><br><br>
			  <a href="#analysis">Results and Analysis</a><br><br>
              <a href="#limitations">Limitations</a><br><br>
			  <a href="#future-work">Future Work</a><br><br>
          </div>
				</div>
				<div class="main-content-block"></div>

					<div class="margin-right-block">
					</div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Introduction</h1>
            <p>Modern deep learning models are heavily overparameterized, often containing more weights and neurons than are needed to solve a task. This leads to unnecessary computation, memory usage, and energy costs. In contrast, biological neural systems such as the human brain achieve remarkable efficiency. During development and learning, the brain prunes both entire neurons and individual synapses, strengthens relevant connections, and maintains sparse architectures that still support flexible and powerful computation. Importantly, this pruning happens throughout learning, not only before or after it
				<span class="citation">[9]
  					<span class="citation-popup">
    					<b>Elimination of Synapses in the Developing Nervous System</b><br>
    						Purves, D., & Lichtman, J. W. (1980). *Science*.
  					</span>
				</span>.
				Biologically-inspired pruning for neural nets is a common idea.</p>
				<p> The biological process has two key components.</p>
				<ul>
					<li>First, entire-neuron pruning occurs early in life, removing whole units. According to neurotrophic theory, the brain first overproduces neurons early in life. Then, developing neurons compete for a limited supply of neurotrophic factor (NTF) hormone
						<span class="citation">[8]
  							<span class="citation-popup">
    							<b>Neurotrophins: Roles in Neuronal Development and Function</b><br>
    								Huang, E. J., & Reichardt, L. F. (2003). *Nature Reviews Neuroscience*.
  							</span>
						</span>.</li>
					<li>Second, synaptic pruning continues throughout adolescence and adulthood. Instead of removing entire neurons, only a subset of the junctions between them are removed. This fine-tunes the connectivity of surviving neurons by eliminating weak or redundant synapses
					<span class="citation">[9]
  						<span class="citation-popup">
    						<b>Elimination of Synapses in the Developing Nervous System</b><br>
    						Purves, D., & Lichtman, J. W. (1980). *Science*.
  						</span>
					</span>. </li>
				</ul>


				</p>In deep learning, these parallel naturally onto <b>structured pruning</b> (removing entire neurons or channels) and <b>unstructured pruning</b> (permanently zeroing individual weights). Structured pruning reduces inference cost because it simplifies the actual dimensions of the model, whereas unstructured pruning typically requires specialized hardware to speed up computation as it does not intrinsically reduce the number of operations.
				</p>Although biological systems use both pruning strategies together and adjust connections dynamically during learning, most deep learning approaches do not. Many pruning methods operate either before training or after training, and dynamic pruning during training is far less explored
				<span class="citation">[3]
  					<span class="citation-popup">
    					<b>A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations</b><br>
    					Cheng, H., Zhang, M., & Shi, J. Q. (2023). arXiv:2308.06767.
  					</span>
				</span> .
				Even methods that do prune during training rarely incorporate biologically grounded rules such as activity-based or energy-based selection. As a result, we still do not know whether using biologically inspired pruning signals during training can produce models that are more efficient or more accurate than those produced by standard pruning techniques.</p>
				</p>To address this, we implemented three biologically motivated mechanisms:
				<ul>
					<li><b>unstructured magnitude pruning</b>, which removes individual weights based on their absolute value</li>
					<li><b>structured energy-based pruning</b>, which removes entire neurons based on the average squared weight magnitude </li>
					<li><b>structured activation-based pruning</b>, which removes entire neurons based on their average activation strength during training </li>
				</ul>
				<p> We compared two ways (energy and activation) of scoring neurons for structured pruning, inspired by how the brain considers both ATP usage and activation frequency.</p>
				<p>These choices reflect the two major forms of pruning observed in the brain and allow us to test whether combining structured and unstructured mechanisms during training yields better results than either method alone
					<span class="citation">[7]
  						<span class="citation-popup">
    						<b>Neuronal Cell Death</b><br>
    							Fricker, M., Tolkovsky, A. M., Borutaite, V., Coleman, M., & Brown, G. C. (2018). *Physiological Reviews*.
  						</span>
					</span>.
					We evaluate all methods consistently on CIFAR-10 to assess sparsity–accuracy trade-offs, training dynamics, and representational changes.</p>
				<p>Ultimately, our goal is to explore whether the principles that guide the efficiency of biological brains can also guide the design of more compact and computationally efficient deep learning architectures.</p>
		    </div>
		    <div class="margin-right-block">
						Margin note that clarifies some detail #main-content-block for intro section.
		    </div>
		</div>
	<div class="content-margin-container" id="background">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Background</h1>
				  	</p>Pruning is a long studied strategy for reducing the size and computational cost of neural networks. A growing body of work shows that many modern neural net parameters are unnecessary for achieving strong performance
					<span class="citation">[6]
  						<span class="citation-popup">
    						<b>What is the State of Neural Network Pruning?</b><br>
    							Blalock, D., Gonzalez Ortiz, J. J., Frankle, J., & Guttag, J. (2020). arXiv:2003.03033.
  						</span>
					</span>.
					This has led to vast literature on identifying smaller subnetworks that retain the accuracy of the full model, often without requiring substantial changes to training pipelines. </p>
				</p>One of the foundational ideas in this area is the Lottery Ticket Hypothesis. Frankle and Carbin (2018) showed that dense, randomly initialized neural networks contain subnetworks (“winning tickets”) that can be trained to reach the same accuracy as the original large model
				<span class="citation">[1]
  					<span class="citation-popup">
    					<b>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</b><br>
    						Frankle, J., & Carbin, M. (2019). arXiv:1803.03635.
  					</span>
				</span>.
				These winning subnetworks are found through iterative magnitude pruning followed by retraining using only the surviving weights, and these subnetworks can match the accuracy of the full model while training in as few or fewer epochs than the original network.</p>
				</p>Follow up work has extended this idea by demonstrating that, in many overparameterized models, subnetworks with competitive performance can be identified even without training
				<span class="citation">[2]
  					<span class="citation-popup">
    					<b>Proving the Lottery Ticket Hypothesis: Pruning is All You Need</b><br>
    						Malach, E., Yehudai, G., Shalev-Shwartz, S., & Shamir, O. (2020). arXiv:2002.00585.
  					</span>
				</span>.
				Recent studies show that optimization procedures can be used to select an efficient subnetwork directly from the initial weights of the model. This suggests that overparameterization creates a large space of viable subnetworks, many of which are structurally efficient.</p>
				</p>Research in pruning has since expanded toward a wide range of techniques
				<span class="citation">[3]
  					<span class="citation-popup">
    					<b>A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations</b><br>
    						Cheng, H., Zhang, M., & Shi, J. Q. (2023). arXiv:2308.06767.
  					</span>
				</span>.
				Pruning methods can be grouped into several major patterns such as loss-based sparsification which encourages sparse weights through modified objectives, sparse-to-sparse approaches that prune and regrow weights during training, filter-level scoring methods for structured pruning, and differentiable mask-learning techniques. These pruning methods are less explored than post-training approaches due to their more complex pruning-regrowth dynamics. Most of the literature has also focused on convolutional networks with considerably fewer studies examining pruning in MLPs or transformers. </p>
				</p>Within unstructured pruning, magnitude pruning remains one of the strongest and simplest baselines. However, more advanced methods exist. For example, Wanda integrates both weights and activation statistics to determine which weights to remove and often outperforms standard magnitude pruning. SparseGPT uses a local reconstruction loss to prune weights while preserving the function of the model, achieving high sparsity with minimal accuracy loss in large language models
				<span class="citation">[10]
  					<span class="citation-popup">
    					<b>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot</b><br>
    						Frantar, E., &amp; Alistarh, D. (2023). arXiv preprint arXiv:2301.00774.
  					</span>
				</span>.
				At the structured level, methods such as coupled structural pruning identify groups of neurons or channels that must be removed together due to architectural dependencies, achieving substantial pruning rates with modest accuracy drops
				<span class="citation">[4]
  					<span class="citation-popup">
    					<b>LLM-Pruner: On the Structural Pruning of Large Language Models</b><br>
    						Ma, X., Fang, G., & Wang, X. (2023). arXiv:2305.11627.
  					</span>
				</span>. </p>
				The majority of structured pruning operates on CNNs, where entire filters can be removed, while very little has been implemented on MLPs.
				<span class="citation">[11]
  					<span class="citation-popup">
    					<b>Data-Driven Sparse Structure Selection for Deep Neural Networks</b><br>
    						Huang, Z., Wang, N. (2018). *Springer*.
  					</span>
				</span> 
				<span class="citation">[12]
  					<span class="citation-popup">
    					<b>Learning Efficient Convolutional Networks through Network Slimming</b><br>
    						Liu, Z., Li, J., Shen, Z., Huang, Gao., Yan, Shoumeng., Zhang, Changshui. (2017). arXiv:1708.06519.
  					</span>
				</span> 
				<span class="citation">[13]
  					<span class="citation-popup">
    					<b>Pruning Filters for Efficient ConvNets</b><br>
    						Li, Hao., Kadav, A., Durdanovic, I., Samet, H., Graf, H. (2016). arXiv:1608.08710.
  					</span>
				</span>. </p>

				</p>Dynamic pruning during training has also been explored but less extensively. Methods such as SET or RigL either add back in weights or limit the number of gradient updates, but does not attempt to alter the dimensionality of the model throughout training. 
				<span class="citation">[5]
  					<span class="citation-popup">
    					<b>Rigging the Lottery: Making All Tickets Winners</b><br>
    						Evci, U., Gale, T., Menick, J., Castro, P. S., & Elsen, E. (2019). arXiv:1911.11134.
  					</span>
				</span>. </p>
				Standardized evaluators such as ShrinkBench ask for masks for the model's weight tensors, but does not intrinsically allow for a way to alter the dimensionality for true structured pruning
				<span class="citation">[6]
 					 <span class="citation-popup">
    					<b>What is the State of Neural Network Pruning?</b><br>
    						Blalock, D., Ortiz, J., Frankle, J., Guttag, J.(2020). arXiv:2003.03033.
  					</span>
				</span>.
				One of the few structured pruning methods that operates on MLPs that we could find (Neuron-level Structured Pruning using Polarization Regularizer) still trains to learn scaling factors on all neurons first, then prunes in one-shot and fine-tunes, rather than removing during initial training
				<span class="citation">[14]
 					 <span class="citation-popup">
    					<b>Neuron-level Structured Pruning using Polarization Regularizer</b><br>
    						Zhuang, T., Zhuang, Z., Huang, Y., Zeng, X., Shuang, K., Li, X. (2020). *NIPS*.
  					</span>
				</span>.
				</p>There remains substantial interest in pruning strategies that are simple and effective during training. In biological neural systems, connections are strengthened, weakened, or removed through mechanisms such as synaptic decay, activity-dependent suppression, and Hebbian plasticity
				<span class="citation">[9]
 					 <span class="citation-popup">
    					<b>Elimination of Synapses in the Developing Nervous System</b><br>
    						Purves, D., & Lichtman, J. W. (1980). *Science*.
  					</span>
				</span>.
				These principles motivate our exploration of energy-based pruning, activity-based pruning, and magnitude pruning. </p>
				<br><br>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

	<div class="content-margin-container" id="hypothesis">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
					<h1>Hypothesis</h1>
					<p>Our hypothesis is that during training, structural pruning will allow the model to keep the neuron structures that matter most, which should reduce computational cost and help preserve accuracy even for small model sizes. We also expect that combining this structured pruning with unstructured magnitude pruning will be even more effective, since each method removes a different type of redundancy in the network. In addition, we predict that a model that prunes itself while it trains can achieve better accuracy-to-size trade-offs than a small model trained from scratch.</p>
					</p>
				<br><br>
		    </div>
		    <div class="margin-right-block" style="transform: translate(0%, -100%);"> <!-- you can move the margin notes up and down with translate -->
          Interestingly, Plato also asked if X does Y, in <a href="#ref_1">[1]</a>.
		    </div>
		</div>

	<div class="content-margin-container" id="methods">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Methods</h1>
						We follow a unified experimental pipeline designed to compare a small baseline model with three methods: magnitude pruning, energy-based structured pruning, and activation-based structured pruning. All models are trained under identical conditions, and every pruning method follows the same pruning schedule to ensure a controlled comparison. <br><br>
						<p><b>General Architecture</b></p>
						<p> All experiments use a custom neural network adapted from the CIFAR-10 model in Homework 1. We chose a simplified architecture to maximize customizability and ensure realistic runtimes on a single T4 GPU in Colab. The model is intentionally made very small so that the effects of parameter reduction are easy to observe. </p>
						<p>The target network consists of:</p>
						<ul>
							<li>an input layer of input size (3072) → 2</b> </li>
							<li>a hidden layer mapping 2 → 2</b> </li>
							<li>a classification layer mapping 2 → 10.</li>
						</ul>
						<p>This architecture contains 6168 parameters.<p>
						</p>The starting network (before pruning) was calculated in order for pruning to produce the target network size after the fixed training schedule. </p>
						<p>The initial network consists of:</p>
						<ul>
							<li>an input layer of input size (3072) → 20</li>
							<li>a hidden layer mapping 20 → 20</li>
							<li>a classification layer mapping 20 → 10.</li>
						</ul>
						<p>This architecture contains 123,480 parameters.</p>
						<p><b>Training Configuration</b></p>
						<p>All models are trained for 30 epochs, with the first 5 epochs serving as a warm-up period with no pruning. From epochs 7 through 30, pruning occurs once after every epoch. Trainings use:</p>
						<ul>
							<li>Optimizer: Stochastic Gradient Descent </li>
							<li>Batch Size: 512</li>
							<li> Loss Function: Cross Entropy Loss </li>
							<li> Learning Rate: 0.15</li>
							<li> Final Pruning Ratio: 0.9</li>
						</ul>
						</p>
						<p>We use a pruning scheduler that computes the pruning ratio for each epoch. This is calculated as: </p>
							<div style="text-align: center;">
  								$$\frac{\text{# of current epochs} - \text{ # of warmup epochs}}
  								{\text{total # of epochs} - \text{# of warmup epochs} - 1}$$
							</div>
							<p>After the warm-up period, the prune ratio increases linearly from 0 to the final target sparsity of <b>90%</b>, ensuring smooth parameter removal over time.</p>
						<b>General Pruning Mechanics</b>
						</p>For all layers, FLOPs are estimated as:</p>
						<div style="text-align: center; margin: 8px 0;">
  							$$\text{FLOPs} = 2 \times (\text{input features}) \times (\text{output features})$$
						</div>

						<b>Neuron and Weight Importance</b>
						<ul>
							<li>Unstructured pruning ranks individual weights by absolute value.</li>
							<li>Structured pruning ranks neurons by the L1 norm of their weights.</li>
							<li>Importance scores are recalculated after every epoch during the pruning phase.</li>
						</ul>
						</p><b>Masking Mechanism</b></p>
						<p>We stored a binary mask for weights or neurons. Both weights and gradients are masked to prevent regrowth, ensuring that once a parameter is pruned, it stays pruned.</p>
						<b>Sparsity Tracking</b>
						<ul>
  							<li>Magnitude pruning:</li>
						</ul>

						<div style="text-align:center; margin: 6px 0 16px 0;">
  							$$\text{sparsity} = \frac{\text{# of zero weights}}{\text{total # of weights}}$$
						</div>

						<ul>
  							<li>Structured pruning:</li>
						</ul>

						<div style="text-align:center; margin: 6px 0 16px 0;">
 							 $$\text{sparsity} = \frac{\text{current parameter count}}
                         {\text{original parameter count}}$$
						</div>
					  </div>

					<div class="margin-right-block">
				</div>
		</div>
	<div class="content-margin-container" id="method-specific-architecture">
		<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Method-Specific Architecture</h1>
						<p><b>Baseline </b></p>
						<p>The target network trained for 30 epochs without pruning.</p>
						<p><b>Magnitude-based</b></p>
						<p> Magnitude pruning identifies the threshold corresponding to the <i>k</i>-th smallest weight value, where: </p>

						<div style="text-align:center; margin: 10px 0;">
  							$$k = (\text{total number of weights}) \times (\text{current prune ratio})$$
						</div>

						<p>
						All weights below this threshold are zeroed and masked.
						</p>
						<p><b>Energy-based</b></p>
						<p>For each neuron, we compute energy as the mean-squared value of its row of weights:</p>

						<div style="text-align:center; margin: 10px 0;">
							$$E_i = \frac{1}{n} \sum_j w_{ij}^2$$
						</div>

						<p>
							Neurons are ranked by this energy score and pruned according to a <i>k</i>-th value threshold.
						</p>

						<p>When a neuron is removed:</p>
						<ul>
							<li>We remove its entire weight row.</li>
							<li>We remove the corresponding column input to the next layer.</li>
							<li>We prune only the first two layers to avoid altering the classification layer.</li>
						</ul>
						<p><b>Activation-Based Structured Pruning</b></p>
						<p>
						This method is the same as energy-based except it instead ranks neurons by how strongly they activate during training.
						For each neuron, we compute an activation score:
						</p>

						<div style="text-align:center; margin: 10px 0;">
							$$
								A_j = \sum_{i=1}^{d_{\text{in}}}
										\left(
										\frac{1}{B} \sum_{b=1}^{B} \left( x_i^{(b)} \right)^2
										\right)
										\cdot |W_{j,i}|
							$$
						</div>

						<p>
							which measures how strongly the neuron responds to the input using its current weight vector.
						</p>
						<p><b>Energy + Magnitude Combined Pruning</b></p>
						<p>
						We use energy and magnitude pruning as specified above.
						We kept our warmup of 5 epochs, then ran 10 epochs of energy (structured)
						pruning and 15 epochs of magnitude (unstructured) pruning.
						We chose to have more unstructured than structured pruning as a parallel
						to how biologically we only remove entire neurons early in the developmental
						process.
						</p>

						<p>
						We had the first round of structured pruning pruned to a final ratio of 0.5,
						while the second round of unstructured pruned to a final ratio of 0.8
						for a combined final sparsity of 0.9 (the same as either method alone
						as specified above.)
						</p>
						<p><b>Activation + Magnitude Combined Pruning</b></p>
						<p>The same as above, except with activation-based scoring instead of energy-based scoring.</p>
		    </div>

					<div class="margin-right-block">
					</div>
		</div>

	<div class="content-margin-container" id="analysis">
    	<div class="margin-left-block"></div>

		<div class="main-content-block">
			<h1>Results & Analysis</h1>
			<p></p>
			<!-- Table 1 + caption -->
			<figure style="text-align: center; margin: 16px 0;">
				<img src="./images/table.png" width="800" style="display:block; margin:auto;">
				<figcaption style="font-style: italic; margin-top: 8px;">
					<b>Figure 1:</b> Accuracy, FLOPs, and # parameters of post-training network
					of different pruning methods averaged over 3 runs.
				</figcaption>
			</figure>
			<p>The validation accuracy of the baseline (training the target network initialized at 6168 parameters) is 19.31% after the full 30 epochs of training. The number of required FLOPs to compute this network is very small at 370,080.</p>
			<p>Our ending parameters with pruning methods are not exact to our target of 6168 parameters due to floating-point inconsistencies, but our ending networks always have a limit of at most 6168. This ensures our pruned networks never have more capacity than the baseline.</p>
			<p>Increasing our model capacity to start at our stated initial network at 123,480 parameters and applying magnitude pruning to have an ending network of 5559 nonzero parameters, we experience a huge increase in validation accuracy (142.9% increase). This is expected as despite the prevalence of zero weights, the dimensionality of our representation is much higher. However, this pruning makes no attempt at reducing the number of FLOPs needed to train, sitting at ~3.7 million FLOPs.</p>
			<p>Moving on to our energy-based pruning, we experience a higher average than the baseline (20.35% increase). We expect this is because our model is implicitly applying the lottery ticket hypothesis. By pruning a large network as we go to a smaller one, we are implicitly searching for good initializations while removing the need to try many initializations or retrain. The accuracy is only about half as good as the non-structured pruning, however, we reduce the number of total required FLOPs by 42.6%.</p>
			<p>Our activation-based pruning performs slightly worse than the energy-based, but still a 12% increase over the baseline. This suggests that using the magnitude of the weights is a better scorer of neurons for pruning than the amount change of the input into that neuron. However, our p value of energy being better than activation is high at 0.097 (not significant) and with only 3 trials, so further testing is needed to be conclusive.</p>
			<p>Combining structured and unstructured pruning methods (energy+magnitude), we preserve a lot more dimensionality than fully structured pruning, while maintaining a similar decrease in FLOPs from unstructured pruning. We experience a 94.6% increase over baseline (preserving 66% of our unstructured pruning accuracy), while reducing FLOPs by 37.43% (preserving 87.9% of the fully structured decrease). This shows that we can preserve the majority of the computation saved from fully structured pruning, and can also preserve a majority of the increase in accuracy from pruning down to the target with fully unstructured pruning.</p>
			<p>When combining activation+magnitude, we experience slightly better accuracy with the same changes in efficiency. This suggests that either the difference between energy or activation observed earlier is not significant, or activation-based pruning is more effective in early epochs.</p>
			<p></p>
			<!-- Image 2 + caption -->
			<figure style="text-align: center; margin: 20px 0;">
				<img src="./images/accuracies.png" width="800" style="display:block; margin:auto;">
				<figcaption style="font-style: italic; margin-top: 8px;">
					<b>Figure 2:</b> Training/validation accuracies and sparsity of one training
					of different pruning methods over 30 epochs.
				</figcaption>
			</figure>
			<p>We observe that the sparsity increases very smoothly with magnitude pruning, as we are picking individual weights to zero. The sparsity jumps a lot more with unstructured pruning, as we remove entire swathes of weights at once at different dimensionalities.</p>
			<p>The difference in seperation between the training and validation accuracy lines can be observed across methods. The seperation is greatest in magnitude pruning, representing how the greater dimensionality and representational power allows for the model to model the training data better and lag on the generalization. The very close lines in the purely structured pruning show the lacking representationality and the subsequent underfitting. The small gap between the two in the combined pruning exhibits how the representationality is smaller than the unstructured pruning but greater than the pure structured pruning.</p>

			<!-- Image 3 + caption -->
			<figure style="text-align: center; margin: 20px 0;">
				<img src="./images/flops.png" width="600" style="display:block; margin:auto;">
				<figcaption style="font-style: italic; margin-top: 8px;">
					<b>Figure 3:</b> FLOPs at each epoch of one training of different pruning methods.
				</figcaption>
			</figure>
			<p>This graph exhibits likely why our combined method is effective. We keep the part of the training loop where the number of FLOPs decreases rapidly, but cut it off when the number of FLOPs reduced begins to plateau. This is because the number of FLOPs saved with each neuron cut decreases as the dimensionality of our model reduces, which shows the tradeoff that stopping earlier can preserve a large amount of representation while maintaining most of the sparsity.</p>
			<!-- Image 4 + caption -->
			<figure style="text-align: center; margin: 20px 0;">
				<img src="./images/neuronsurvival.png" width="800" style="display:block; margin:auto;">
				<figcaption style="font-style: italic; margin-top: 8px;">
					<b>Figure 4:</b> Neuron importance at each epoch of one training of different pruning methods.
				</figcaption>
			</figure>
			<p>This shows a pattern similar to Figure 3: in the combined pruning method, once most neurons become highly important, structured pruning effectively stops. </p>
		<div class="margin-right-block"></div>
	</div>
	</div>

	<div class="content-margin-container" id="limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Limitations</h1>
						<p>This study also has several limitations that place constraints on the conclusions we can make.
							<ul>
								<li><b>Datasets</b>: Our experiments rely on small models and datasets such as CIFAR-10, which have limited resolution and a small number of categories. These datasets do not fully capture the complexity of large-scale vision tasks, so pruning behavior may differ on more challenging benchmarks. </li>
								<br>
								<li><b>Compute Constraints</b>: All experiments were run in Google Colab on a single T4 GPU, which limited the range of capability we could test and hyperparameters we could explore.</li>
								<br>
								<li><b>Constrained Baseline:</b> We have severely bottlenecked our baseline in order to compare the effects of pruning to constraints. Our representational capability at two dimensions is very weak, and thus shows a poor performance baseline (~20%). Similarly, our training and validation accuracies are very close together, suggesting we are underfitting. Our difference in performance between energy and activation-based pruning is small, and would require more runs to verify.</li>
							<br>
							<li><b>Limited Training Trials:</b> Due to the presence of effects such as the lottery ticket hypothesis, we should retrain the model more than three times each in order to get true observations, especially as with the tiny number of parameters individual initial initializations may have a large impact on the outcome.</li>
							<br>
							<li><b>Time Constraints:</b> Time limited our ability to test additional biological mechanisms, to vary architectural choices, or to perform deeper analyses of representation stability.</li>
								<br>
								<li><b>Model Implementation:</b> We have used a model implemented from ground-up, limiting the significance of our research on state-of-the-art models.</li>
								<br>
								<li><b>Baseline Comparision:</b> Lacking established structured during-training MLP pruning methods makes it more difficult to properly compare and benchmark our method to existing methods. </li>
							</ul>


						<p></p>


						<p>Overall, these limitations mean that our findings should be interpreted as a start, and future work with larger models, more varied datasets, and greater computational resources would help determine how these biologically inspired methods can be applied.</p>
            <p></p>
		    </div>

					<div class="margin-right-block">
					</div>
		</div>

	<div class="content-margin-container" id="future-work">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Future Work</h1>
						<p>Future work could include testing the optimal ratio of structured to unstructured pruning over the total epochs. It would also be helpful to have a deeper comparision of during-training pruning versus one-shot pruning. We could experiement with additional biologically inspired scoring rules such as the Hebbian theory, which looks at how pairs of neurons activate together. Finally, it would good to try these methods on larger, state-of-the-art models to see how well the ideas scale.  </p>
						<p>Exploring specialized sparse matrix multiplication techniques could make use of the zeroed magnitude pruning weights as well to further reduce FLOPs or make a comparision of FLOP reduction between structured and unstructured pruning. A comparision of structured+unstructured pruning with pure unstructured pruning with sparse matrix acceleration would be helpful.</p>

		    </div>

					<div class="margin-right-block">
					</div>
		</div>

	</body>
	<div class="content-margin-container" id="references">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>References</h1>
            <p>
				<ol>

					<br>
						<li><b>The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks</b>
						<br>Frankle, J., & Carbin, M., 2019. arXiv preprint arXiv:1803.03635.</li>
					<br>
						<li><b>Proving the Lottery Ticket Hypothesis: Pruning is All You Need</b>
						<br>Malach, E., Yehudai, G., Shalev-Shwartz, S., & Shamir, O., 2020. arXiv preprint arXiv:2002.00585.</li>
					<br>
					<li><b>A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations</b>
						<br>Cheng, H., Zhang, M., & Shi, J. Q., 2023. arXiv preprint arXiv:2308.06767.
					</li>
					<br>
					<li><b>LLM-Pruner: On the Structural Pruning of Large Language Models</b>
						<br>Ma, X., Fang, G., & Wang, X., 2023. arXiv preprint arXiv:2305.11627.
					</li>
					<br>
					<li><b>Rigging the Lottery: Making All Tickets Winners</b>
						<br>Evci, U., Gale, T., Menick, J., Castro, P. S., & Elsen, E., 2019. arXiv preprint arXiv:1911.11134.
					</li>
					<br>
					<li>
						<b>What is the State of Neural Network Pruning?</b>
						<br>Blalock, D., Gonzalez Ortiz, J. J., Frankle, J., & Guttag, J., 2020. arXiv preprint arXiv:2003.03033.

					</li>
					<br>
					<li><b>Neuronal Cell Death</b>
						<br>Fricker, M., Tolkovsky, A. M., Borutaite, V., Coleman, M., & Brown, G. C., 2018. Physiological Reviews.

					</li>
					<br>
					<li>
						<b>Neurotrophins: Roles in Neuronal Development and Function</b>
						<br>Huang, E. J., & Reichardt, L. F., 2003. Nature Reviews Neuroscience.

					</li>
					<br>
					<li>
						<b>Elimination of Synapses in the Developing Nervous System</b>
						<br>Purves, D., & Lichtman, J. W., 1980. Science.

					</li>
					<br>
					<li>
  						<b>SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot</b>
  						<br>Frantar, E., & Alistarh, D., 2023. arXiv preprint arXiv:2301.00774.
					</li>
					<br>
					<li><b>Data-Driven Sparse Structure Selection for Deep Neural Networks</b>
						<br>Huang, Z., Wang, N. (2018). *Springer*.
					</li>
					<br>
					<li><b>Learning Efficient Convolutional Networks through Network Slimming</b>
						<br>Liu, Z., Li, J., Shen, Z., Huang, Gao., Yan, Shoumeng., Zhang, Changshui. (2017). arXiv:1708.06519.
					</li>
					<br>
					<li><b>Pruning Filters for Efficient ConvNets</b>
						<br>Li, Hao., Kadav, A., Durdanovic, I., Samet, H., Graf, H. (2016). arXiv:1608.08710.
					</li>
					<br>
					<li><b>Neuron-level Structured Pruning using Polarization Regularizer</b>
						<br>Zhuang, T., Zhuang, Z., Huang, Y., Zeng, X., Shuang, K., Li, X. (2020). *NIPS*.
					</li>
					<br>
				</ol>
			</p>
		    </div>

					<div class="margin-right-block">
					</div>
		</div>

	</body>


</html>
